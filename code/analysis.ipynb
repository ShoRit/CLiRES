{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import *\n",
    "\n",
    "data_dir = '/data/multilingual_KGQA/IndoRE/data'\n",
    "data     = load_dill(f'{data_dir}/english_rels.dill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['tokens', 'tok_range', 'arg1_ids', 'arg2_ids', 'arg1_type', 'arg2_type', 'desc_emb', 'label', 'dep_data', 'sent'])\n"
     ]
    }
   ],
   "source": [
    "print(data['train']['rels'][0].keys())\n",
    "\n",
    "ele = data['train']['rels'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[30, 768], edge_index=[2, 29], edge_type=[29], n1_mask=[30], n2_mask=[30])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ele['dep_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
       "         19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29],\n",
       "        [10,  3,  1,  6,  6,  3,  6,  3, 10,  0, 14, 14, 14, 10, 16, 14, 18, 16,\n",
       "         20, 14, 23, 23, 20, 28, 28, 28, 28, 20, 10]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ele['dep_data'].edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_dict             \t= \tload_pickle(f'{data_dir}/ents.pkl')\n",
    "relation_dict          \t\t= \tload_pickle(f'{data_dir}/rels.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'manner_of_death': 0,\n",
       " 'position_held': 1,\n",
       " 'child': 2,\n",
       " 'producer': 3,\n",
       " 'contains_administrative_territorial_entity': 4,\n",
       " 'ethnic_group': 5,\n",
       " 'member_of_political_party': 6,\n",
       " 'member_of_sports_team': 7,\n",
       " 'founded_by': 8,\n",
       " 'mother': 9,\n",
       " 'employer': 10,\n",
       " 'educated_at': 11,\n",
       " 'present_in_work': 12,\n",
       " 'participant': 13,\n",
       " 'tributary': 14,\n",
       " 'place_of_death': 15,\n",
       " 'publisher': 16,\n",
       " 'creator': 17,\n",
       " 'member_of': 18,\n",
       " 'subsidiary': 19,\n",
       " 'owned_by': 20,\n",
       " 'parent_organization': 21,\n",
       " 'material_used': 22,\n",
       " 'writing_system': 23,\n",
       " 'named_after': 24,\n",
       " 'genre': 25,\n",
       " 'performer': 26,\n",
       " 'composer': 27,\n",
       " 'continent': 28,\n",
       " 'color': 29,\n",
       " 'discoverer_or_inventor': 30,\n",
       " 'director': 31,\n",
       " 'screenwriter': 32,\n",
       " 'shares_border_with': 33,\n",
       " 'place_of_birth': 34,\n",
       " 'award_received': 35,\n",
       " 'nominated_for': 36,\n",
       " 'production_company': 37,\n",
       " 'located_in_or_next_to_body_of_water': 38,\n",
       " 'occupation': 39,\n",
       " 'student_of': 40,\n",
       " 'capital_of': 41,\n",
       " 'spouse': 42,\n",
       " 'country_of_citizenship': 43,\n",
       " 'father': 44,\n",
       " 'capital': 45,\n",
       " 'winner': 46,\n",
       " 'league': 47,\n",
       " 'sibling': 48,\n",
       " 'original_language_of_film_or_TV_show': 49,\n",
       " 'sport': 50}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relation_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x, edge_index, edge_type, n1_mask, n2_mask\t\t= [],[[],[]],[],[],[]\n",
    "\n",
    "x.append([0 for i in range(0,768)])\n",
    "n1_mask.append(0)\n",
    "n2_mask.append(0)\n",
    "\n",
    "x, edge_index, edge_type, n1_mask, n2_mask\t\t= torch.FloatTensor(np.array(x)), torch.LongTensor(edge_index), torch.LongTensor(edge_type), torch.LongTensor(n1_mask),torch.LongTensor(n2_mask)\n",
    "dep_data \t\t\t\t\t\t\t\t\t\t= Data(x=x, edge_index= edge_index, edge_type=edge_type, n1_mask=n1_mask, n2_mask=n2_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=2 python3 relextract.py --src_lang english --tgt_lang  english --mode train --mtl 0.5 \n",
    "\n",
    "CUDA_VISIBLE_DEVICES=1 python3 relextract.py --src_lang hindi   --tgt_lang  hindi   --mode train --mtl 0.5 --batch_size 6 &\n",
    "CUDA_VISIBLE_DEVICES=2 python3 relextract.py --src_lang telugu  --tgt_lang  telugu  --mode train --mtl 0.5 --batch_size 6 &\n",
    "\n",
    "CUDA_VISIBLE_DEVICES=3 python3 relextract.py --src_lang hindi   --tgt_lang  hindi   --mode train --mtl 0.5 --batch_size 6 --dep 0&\n",
    "CUDA_VISIBLE_DEVICES=4 python3 relextract.py --src_lang telugu  --tgt_lang  telugu  --mode train --mtl 0.5 --batch_size 6 --dep 0&\n",
    "\n",
    "CUDA_VISIBLE_DEVICES=5 python3 relextract.py --src_lang bengali  --tgt_lang  bengali  --mode train --mtl 0.5 --batch_size 6 --dep 0&\n",
    "CUDA_VISIBLE_DEVICES=0 python3 relextract.py --src_lang english  --tgt_lang  english  --mode train --mtl 0.5 --batch_size 6 --dep 0&\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### COMPLETE ZS Transfer here\n",
    "\n",
    "CUDA_VISIBLE_DEVICES=3 python3 relextract.py --src_lang english  --tgt_lang  hindi  --mode eval --mtl 0.5 --batch_size 8 --dep 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'has-type': 0, 'has-occupation': 1, 'no_relation': 2, 'has-genre': 3, 'event-year': 4, 'has-population': 5, 'has-edu': 6, 'is-member-of': 7, 'movie-has-director': 8, 'birth-place': 9, 'org-has-founder': 10, 'is-where': 11, 'from-country': 12, 'has-author': 13, 'org-has-member': 14, 'has-parent': 15, 'has-child': 16, 'headquarters': 17, 'loc-leader': 18, 'org-leader': 19, 'has-spouse': 20, 'won-award': 21, 'invented-by': 22, 'invented-when': 23, 'has-nationality': 24, 'starring': 25, 'has-sibling': 26, 'has-weight': 27, 'has-height': 28, 'has-length': 29, 'has-highest-mountain': 30, 'first-product': 31, 'has-tourist-attraction': 32, 'has-lifespan': 33, 'eats': 34, 'post-code': 35}\n",
      "Counter({'birth-place': 110310, 'has-occupation': 68527, 'is-where': 62329, 'has-type': 60826, 'movie-has-director': 54655, 'from-country': 34503, 'has-genre': 34354, 'has-population': 33485, 'has-author': 29557, 'is-member-of': 20085, 'headquarters': 19341, 'org-has-member': 17812, 'has-parent': 9263, 'org-has-founder': 8733, 'has-spouse': 6008, 'no_relation': 5127, 'has-edu': 4562, 'org-leader': 4210, 'won-award': 4107, 'has-nationality': 3197, 'event-year': 3176, 'has-child': 3121, 'starring': 2656, 'has-sibling': 1007, 'has-length': 633, 'invented-when': 627, 'has-tourist-attraction': 570, 'has-highest-mountain': 542, 'first-product': 493, 'has-lifespan': 486, 'invented-by': 475, 'has-height': 470, 'loc-leader': 430, 'has-weight': 427, 'post-code': 278, 'eats': 105})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from helper import *\n",
    "from pprint import pprint\n",
    "\n",
    "rel_dir                     = '/data/multilingual_KGQA/SMILER/data'\n",
    "files \t\t\t\t\t\t= os.listdir(f'{rel_dir}')\n",
    "languages \t\t\t\t\t= set([file.split('/')[-1][0:2] for file in files if file.endswith('tsv')])\n",
    "\n",
    "lang_dict                   = {}\n",
    "langs                       = []\n",
    "\n",
    "for lang in languages:\n",
    "    lang_df \t\t\t\t= pd.read_csv(f'{rel_dir}/{lang}_corpora_train.tsv', sep='\\t')\n",
    "    curr_langs \t\t\t\t= list(lang_df['label'])\n",
    "    langs.extend(curr_langs)\n",
    "    \n",
    "for lang in langs:\n",
    "    # if lang == 'no_relation': continue\n",
    "    if lang not in lang_dict: lang_dict[lang] = len(lang_dict)\n",
    "\n",
    "print(lang_dict)\n",
    "print(Counter(langs))\n",
    "\n",
    "test_langs                  = []\n",
    "test_lang_dict              = {}\n",
    "for lang in languages:\n",
    "    lang_df \t\t\t\t= pd.read_csv(f'{rel_dir}/{lang}_corpora_test.tsv', sep='\\t')\n",
    "    curr_langs \t\t\t\t= list(lang_df['label'])\n",
    "    test_langs.extend(curr_langs)\n",
    "    \n",
    "for lang in test_langs:\n",
    "    # if lang == 'no_relation': continue\n",
    "    if lang not in test_lang_dict: test_lang_dict[lang] = len(test_lang_dict)\n",
    "    \n",
    "len(lang_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>entity_1</th>\n",
       "      <th>entity_2</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39952</td>\n",
       "      <td>Arturo Sosa</td>\n",
       "      <td>Venezuela</td>\n",
       "      <td>from-country</td>\n",
       "      <td>&lt;e1&gt;Arturo Sosa&lt;/e1&gt; Abascal (Caracas, &lt;e2&gt;Ven...</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41033</td>\n",
       "      <td>I Want You So Bad</td>\n",
       "      <td>Rhythm and blues</td>\n",
       "      <td>has-genre</td>\n",
       "      <td>\" &lt;e1&gt;I Want You So Bad&lt;/e1&gt; \" é uma canção &lt;e...</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9908</td>\n",
       "      <td>Abadiño</td>\n",
       "      <td>Espanha</td>\n",
       "      <td>is-where</td>\n",
       "      <td>Abadiano (em ) ou &lt;e1&gt;Abadiño&lt;/e1&gt; (em ) é um ...</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26966</td>\n",
       "      <td>Steinsberg</td>\n",
       "      <td>Município</td>\n",
       "      <td>has-type</td>\n",
       "      <td>&lt;e1&gt;Steinsberg&lt;/e1&gt; é um &lt;e2&gt;município&lt;/e2&gt; da...</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19673</td>\n",
       "      <td>Janela Secreta</td>\n",
       "      <td>David Koepp</td>\n",
       "      <td>movie-has-director</td>\n",
       "      <td>A &lt;e1&gt;Janela Secreta&lt;/e1&gt; () é um filme de sus...</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>43228</td>\n",
       "      <td>Playgirl</td>\n",
       "      <td>Joseph Pevney</td>\n",
       "      <td>movie-has-director</td>\n",
       "      <td>&lt;e1&gt;Playgirl&lt;/e1&gt; (bra Amantes por uma Noite )...</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881</th>\n",
       "      <td>28287</td>\n",
       "      <td>Stromae</td>\n",
       "      <td>Compositor</td>\n",
       "      <td>has-occupation</td>\n",
       "      <td>&lt;e1&gt;Stromae&lt;/e1&gt; (pronuncia-se Stromaê ou em a...</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>42527</td>\n",
       "      <td>Luigi Di Maio</td>\n",
       "      <td>Itália</td>\n",
       "      <td>birth-place</td>\n",
       "      <td>&lt;e1&gt;Luigi Di Maio&lt;/e1&gt; (; nascido em 6 de julh...</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>41313</td>\n",
       "      <td>Andreas Kuffner</td>\n",
       "      <td>Munique</td>\n",
       "      <td>birth-place</td>\n",
       "      <td>&lt;e1&gt;Andreas Kuffner&lt;/e1&gt; (&lt;e2&gt;Munique&lt;/e2&gt;, 30...</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884</th>\n",
       "      <td>11881</td>\n",
       "      <td>Knutwil</td>\n",
       "      <td>Suíça</td>\n",
       "      <td>is-where</td>\n",
       "      <td>&lt;e1&gt;Knutwil&lt;/e1&gt; é uma comuna da &lt;e2&gt;Suíça&lt;/e2...</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>885 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id           entity_1          entity_2               label  \\\n",
       "0    39952        Arturo Sosa         Venezuela        from-country   \n",
       "1    41033  I Want You So Bad  Rhythm and blues           has-genre   \n",
       "2     9908            Abadiño           Espanha            is-where   \n",
       "3    26966         Steinsberg         Município            has-type   \n",
       "4    19673     Janela Secreta       David Koepp  movie-has-director   \n",
       "..     ...                ...               ...                 ...   \n",
       "880  43228           Playgirl     Joseph Pevney  movie-has-director   \n",
       "881  28287            Stromae        Compositor      has-occupation   \n",
       "882  42527      Luigi Di Maio            Itália         birth-place   \n",
       "883  41313    Andreas Kuffner           Munique         birth-place   \n",
       "884  11881            Knutwil             Suíça            is-where   \n",
       "\n",
       "                                                  text lang  \n",
       "0    <e1>Arturo Sosa</e1> Abascal (Caracas, <e2>Ven...   pt  \n",
       "1    \" <e1>I Want You So Bad</e1> \" é uma canção <e...   pt  \n",
       "2    Abadiano (em ) ou <e1>Abadiño</e1> (em ) é um ...   pt  \n",
       "3    <e1>Steinsberg</e1> é um <e2>município</e2> da...   pt  \n",
       "4    A <e1>Janela Secreta</e1> () é um filme de sus...   pt  \n",
       "..                                                 ...  ...  \n",
       "880  <e1>Playgirl</e1> (bra Amantes por uma Noite )...   pt  \n",
       "881  <e1>Stromae</e1> (pronuncia-se Stromaê ou em a...   pt  \n",
       "882  <e1>Luigi Di Maio</e1> (; nascido em 6 de julh...   pt  \n",
       "883  <e1>Andreas Kuffner</e1> (<e2>Munique</e2>, 30...   pt  \n",
       "884  <e1>Knutwil</e1> é uma comuna da <e2>Suíça</e2...   pt  \n",
       "\n",
       "[885 rows x 6 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# lang_df[lang_df.label!='no_relation']\n",
    "lang_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickle dumped successfully\n"
     ]
    }
   ],
   "source": [
    "dump_pickle(lang_dict, '/data/multilingual_KGQA/SMILER/data/rels.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 16:55:14 INFO: Loading these models for language: hi (Hindi):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | hdtb    |\n",
      "| pos       | hdtb    |\n",
      "| lemma     | hdtb    |\n",
      "| depparse  | hdtb    |\n",
      "=======================\n",
      "\n",
      "2022-04-22 16:55:14 INFO: Use device: gpu\n",
      "2022-04-22 16:55:14 INFO: Loading: tokenize\n",
      "2022-04-22 16:55:14 INFO: Loading: pos\n",
      "2022-04-22 16:55:15 INFO: Loading: lemma\n",
      "2022-04-22 16:55:15 INFO: Loading: depparse\n",
      "2022-04-22 16:55:15 INFO: Done loading processors!\n",
      "100%|██████████| 5570/5570 [05:38<00:00, 16.45it/s]\n",
      "100%|██████████| 696/696 [00:41<00:00, 16.88it/s]\n",
      "100%|██████████| 697/697 [00:41<00:00, 16.77it/s]\n",
      "2022-04-22 17:02:21 INFO: Loading these models for language: en (English):\n",
      "============================\n",
      "| Processor    | Package   |\n",
      "----------------------------\n",
      "| tokenize     | combined  |\n",
      "| pos          | combined  |\n",
      "| lemma        | combined  |\n",
      "| depparse     | combined  |\n",
      "| sentiment    | sstplus   |\n",
      "| constituency | wsj       |\n",
      "| ner          | ontonotes |\n",
      "============================\n",
      "\n",
      "2022-04-22 17:02:21 INFO: Use device: gpu\n",
      "2022-04-22 17:02:21 INFO: Loading: tokenize\n",
      "2022-04-22 17:02:21 INFO: Loading: pos\n",
      "2022-04-22 17:02:22 INFO: Loading: lemma\n",
      "2022-04-22 17:02:22 INFO: Loading: depparse\n",
      "2022-04-22 17:02:22 INFO: Loading: sentiment\n",
      "2022-04-22 17:02:23 INFO: Loading: constituency\n",
      "2022-04-22 17:02:23 INFO: Loading: ner\n",
      "2022-04-22 17:02:24 INFO: Done loading processors!\n",
      "100%|██████████| 6788/6788 [23:20<00:00,  4.85it/s]\n",
      "100%|██████████| 849/849 [02:57<00:00,  4.79it/s]\n",
      "100%|██████████| 849/849 [02:53<00:00,  4.89it/s]\n",
      "2022-04-22 17:31:42 INFO: Loading these models for language: te (Telugu):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | mtg      |\n",
      "| pos       | mtg      |\n",
      "| lemma     | identity |\n",
      "| depparse  | mtg      |\n",
      "========================\n",
      "\n",
      "2022-04-22 17:31:42 INFO: Use device: gpu\n",
      "2022-04-22 17:31:42 INFO: Loading: tokenize\n",
      "2022-04-22 17:31:42 INFO: Loading: pos\n",
      "2022-04-22 17:31:43 INFO: Loading: lemma\n",
      "2022-04-22 17:31:43 INFO: Loading: depparse\n",
      "2022-04-22 17:31:43 INFO: Done loading processors!\n",
      "100%|██████████| 6523/6523 [04:38<00:00, 23.46it/s]\n",
      "100%|██████████| 815/815 [00:35<00:00, 23.23it/s]\n",
      "100%|██████████| 816/816 [00:34<00:00, 23.86it/s]\n"
     ]
    }
   ],
   "source": [
    "from helper import *\n",
    "rel_dir                     = '/data/multilingual_KGQA/IndoRE/data'\n",
    "langs                       = ['hindi','english','telugu']\n",
    "lang_code                   = {'hindi':'hi', 'english':'en', 'telugu':'te'}\n",
    "import stanza\n",
    "sent_count_dict             = ddict(lambda:ddict(int))\n",
    "\n",
    "for lang in langs:\n",
    "    data_dict               = load_dill(f'{rel_dir}/{lang}_rels.dill')\n",
    "    stanza_nlp \t\t\t\t= stanza.Pipeline(lang=lang_code[lang])    \n",
    "    for split in data_dict:\n",
    "        for elem in tqdm(data_dict[split]['rels']):\n",
    "            dep_doc         = stanza_nlp(elem['sent'])\n",
    "            sent_count_dict[lang][len(dep_doc.sentences)]+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function <lambda> at 0x7f34f14f9ee0>, {'hindi': defaultdict(<class 'int'>, {1: 5590, 2: 765, 4: 156, 6: 54, 3: 231, 5: 122, 8: 12, 7: 24, 9: 6, 10: 2, 11: 1}), 'english': defaultdict(<class 'int'>, {1: 8311, 2: 172, 3: 3}), 'telugu': defaultdict(<class 'int'>, {1: 471, 5: 1037, 3: 1153, 4: 1103, 6: 886, 2: 958, 7: 655, 10: 298, 8: 556, 9: 380, 11: 207, 12: 148, 17: 22, 20: 12, 18: 13, 13: 97, 15: 39, 19: 8, 16: 25, 14: 62, 21: 6, 23: 5, 27: 2, 31: 2, 26: 4, 25: 2, 24: 1, 22: 1, 40: 1})})\n"
     ]
    }
   ],
   "source": [
    "print(sent_count_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1, 1)\n",
      "(2, 15, 2)\n",
      "(16, 21, 3)\n",
      "(22, 30, 4)\n",
      "(31, 37, 5)\n",
      "(38, 42, 6)\n",
      "(43, 47, 7)\n",
      "(48, 54, 8)\n",
      "(55, 61, 9)\n",
      "(62, 70, 10)\n",
      "(71, 79, 11)\n",
      "(80, 84, 12)\n",
      "(85, 93, 13)\n",
      "(93, 94, 14)\n",
      "(95, 101, 1)\n",
      "(102, 113, 2)\n",
      "(114, 123, 3)\n",
      "(124, 127, 4)\n",
      "(128, 138, 5)\n",
      "(138, 139, 6)\n",
      "(140, 153, 7)\n",
      "(153, 154, 8)\n",
      "(155, 164, 9)\n",
      "(165, 169, 10)\n",
      "(170, 173, 11)\n",
      "(174, 179, 12)\n",
      "(180, 184, 13)\n",
      "(185, 191, 14)\n",
      "(192, 197, 15)\n",
      "(198, 207, 16)\n",
      "(208, 213, 17)\n",
      "(214, 221, 18)\n",
      "(222, 231, 19)\n",
      "(232, 239, 20)\n",
      "(239, 240, 21)\n",
      "(241, 243, 1)\n",
      "(244, 253, 2)\n",
      "(254, 258, 3)\n",
      "(259, 268, 4)\n",
      "(269, 273, 5)\n",
      "(274, 290, 6)\n",
      "(290, 291, 7)\n",
      "(292, 296, 1)\n",
      "(297, 299, 2)\n",
      "(300, 306, 3)\n",
      "(307, 311, 4)\n",
      "(312, 317, 5)\n",
      "(317, 318, 6)\n",
      "(319, 324, 7)\n",
      "(325, 330, 8)\n",
      "(331, 335, 9)\n",
      "(336, 341, 10)\n",
      "(342, 347, 11)\n",
      "(347, 348, 12)\n",
      "(349, 357, 13)\n",
      "(358, 364, 14)\n",
      "(364, 365, 15)\n",
      "(366, 373, 1)\n",
      "(374, 378, 2)\n",
      "(379, 382, 3)\n",
      "(383, 387, 4)\n",
      "(388, 398, 5)\n",
      "(399, 404, 6)\n",
      "(405, 411, 7)\n",
      "(412, 419, 8)\n",
      "(420, 435, 9)\n",
      "(435, 436, 10)\n",
      "ఆ మహిమాన్వితమైన పాయసం సేవించడం ద్వారా అంజన కోతి ముఖంగల శివుని అవతారమైన శిశువుకి జన్మ ఇచ్చింది. బాలుడు అంజనాదేవికి జన్మించటం వలన ఆంజనేయుడని, కేసరినందనుడని, వాయుపుత్ర లేదా పవన పుత్ర అంటే వాయువు యొక్క కుమారుడని వివిధ పేర్లతో ప్రసిద్ధి చెందాడు. తన బాల్యదశలో కూడా హనుమంతుడు చాలా శక్తివంతమైనవాడు. అతను తన తండ్రి అయిన కేసరి, తల్లి అప్సర అంజన యొక్క శక్తి, వాయువేగం గలవాడు. హనుమాన్ జననం వలన అంజన శాపవిమోచనం పొంది తిరిగి స్వర్గం తిరిగివెళ్ళింది.\n"
     ]
    }
   ],
   "source": [
    "dep_doc         = stanza_nlp(elem['sent'])\n",
    "for sent in dep_doc.sentences:\n",
    "    for word in sent.words:\n",
    "        print((word.start_char, word.end_char, word.id))\n",
    "\n",
    "print(elem['sent'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incorporate the EL tensors for RE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import *\n",
    "import ast\n",
    "el_dir      = '/data/multilingual_KGQA/IndoRE/data/el_data'\n",
    "data_dir    = '/data/multilingual_KGQA/IndoRE/data/'\n",
    "langs       = ['hindi','english','bengali','telugu']\n",
    "\n",
    "lang_codes = {'hi':'hindi', 'en':'english','te': 'telugu'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6963it [00:00, 374072.20it/s], ?it/s]\n",
      " 33%|███▎      | 1/3 [00:00<00:00,  4.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8486it [00:00, 605866.92it/s]\n",
      " 67%|██████▋   | 2/3 [00:00<00:00,  4.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8154it [00:00, 309639.98it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00,  4.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "te 3\n",
      "Pickle dumped successfully\n"
     ]
    }
   ],
   "source": [
    "split_langs \t=    f'{data_dir}/lang_sents.dill'\n",
    "\n",
    "uniq_dict       =   ddict(dict)\n",
    "for lang in tqdm(lang_codes):\n",
    "    cnt                                     =    0\n",
    "    lang_data                               =    load_pickle(f'{el_dir}/{lang_codes[lang]}_indore_el_new_5m.pkl')\n",
    "    lines \t\t                            =    open(f'{data_dir}/{lang_codes[lang]}_indore.tsv').readlines()\n",
    "    \n",
    "    for line, elem in tqdm(zip(lines, lang_data)):\n",
    "        rel, sent, ent_1, ent_2             =    line.strip().split('\\t')\n",
    "        # sent                                =    sent.replace('<e1>','').replace('</e1>','').replace('<e2>','').replace('</e2>','')\n",
    "        if sent in uniq_dict[lang]:\n",
    "            cnt +=1\n",
    "        uniq_dict[lang][sent]               =    (elem[0], elem[1])\n",
    "    \n",
    "    print(lang, cnt)\n",
    "    \n",
    "dump_pickle(uniq_dict, f'{data_dir}/ent2_embs.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5570/5570 [00:00<00:00, 53854.94it/s]\n",
      "100%|██████████| 697/697 [00:00<00:00, 347448.29it/s]\n",
      "100%|██████████| 696/696 [00:00<00:00, 314065.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dill dumped successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6523/6523 [00:00<00:00, 49417.66it/s]\n",
      "100%|██████████| 816/816 [00:00<00:00, 279620.27it/s]\n",
      "100%|██████████| 815/815 [00:00<00:00, 280515.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dill dumped successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6788/6788 [00:00<00:00, 44251.03it/s]\n",
      "100%|██████████| 849/849 [00:00<00:00, 116741.44it/s]\n",
      "100%|██████████| 849/849 [00:00<00:00, 146977.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dill dumped successfully\n"
     ]
    }
   ],
   "source": [
    "for lang in ['hi','te','en']:\n",
    "    rels_data = load_dill(f'{data_dir}/{lang}_rels.dill')\n",
    "    lang_cnts = 0\n",
    "    data   \t\t\t        \t\t\t= ddict(lambda: ddict(list))\n",
    "    for split in ['train','test','dev']:\n",
    "        for elem in tqdm(rels_data[split]['rels']):\n",
    "            curr_data                = elem\n",
    "            sent, arg1_emb, arg2_emb = elem['orig_sent'], elem['arg1_emb'], elem['arg2_emb']\n",
    "            if sent not in uniq_dict[lang]: lang_cnts+=1\n",
    "            # for key_ in  elem:\n",
    "            #     data[split]['rels'][key_] = elem[key_]\n",
    "            arg1_emb, arg2_emb    = uniq_dict[lang][sent]\n",
    "            if isinstance(arg1_emb, int) is False : curr_data['arg1_emb'] = arg1_emb\n",
    "            if isinstance(arg2_emb, int) is False : curr_data['arg2_emb'] = arg2_emb       \n",
    "            data[split]['rels'].append(curr_data)\n",
    "    dump_dill(data, f'{data_dir}/{lang}_ent_rels.dill')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'has-type': 0, 'birth-place': 1, 'has-occupation': 2, 'has-genre': 3, 'has-author': 4, 'org-has-member': 5, 'is-where': 6, 'movie-has-director': 7, 'has-population': 8, 'has-edu': 9, 'headquarters': 10, 'from-country': 11, 'org-leader': 12, 'is-member-of': 13, 'org-has-founder': 14, 'loc-leader': 15, 'has-spouse': 16, 'has-parent': 17, 'event-year': 18, 'has-child': 19, 'won-award': 20, 'has-sibling': 21, 'has-nationality': 22, 'first-product': 23, 'invented-when': 24, 'invented-by': 25, 'has-highest-mountain': 26, 'has-tourist-attraction': 27, 'starring': 28, 'has-weight': 29, 'has-height': 30, 'has-length': 31, 'has-lifespan': 32, 'eats': 33, 'post-code': 34}\n",
      "Counter({'birth-place': 110310, 'has-occupation': 68527, 'is-where': 62329, 'has-type': 60826, 'movie-has-director': 54655, 'from-country': 34503, 'has-genre': 34354, 'has-population': 33485, 'has-author': 29557, 'is-member-of': 20085, 'headquarters': 19341, 'org-has-member': 17812, 'has-parent': 9263, 'org-has-founder': 8733, 'has-spouse': 6008, 'no_relation': 5127, 'has-edu': 4562, 'org-leader': 4210, 'won-award': 4107, 'has-nationality': 3197, 'event-year': 3176, 'has-child': 3121, 'starring': 2656, 'has-sibling': 1007, 'has-length': 633, 'invented-when': 627, 'has-tourist-attraction': 570, 'has-highest-mountain': 542, 'first-product': 493, 'has-lifespan': 486, 'invented-by': 475, 'has-height': 470, 'loc-leader': 430, 'has-weight': 427, 'post-code': 278, 'eats': 105})\n"
     ]
    }
   ],
   "source": [
    "from helper import *\n",
    "from pprint import pprint\n",
    "\n",
    "rel_dir                     = '/data/multilingual_KGQA/SMILER/data'\n",
    "files \t\t\t\t\t\t= os.listdir(f'{rel_dir}')\n",
    "languages \t\t\t\t\t= set([file.split('/')[-1][0:2] for file in files if file.endswith('tsv')])\n",
    "\n",
    "lang_dict                   = {}\n",
    "langs                       = []\n",
    "\n",
    "for lang in languages:\n",
    "    lang_df \t\t\t\t= pd.read_csv(f'{rel_dir}/{lang}_corpora_train.tsv', sep='\\t')\n",
    "    curr_langs \t\t\t\t= list(lang_df['label'])\n",
    "    langs.extend(curr_langs)\n",
    "    \n",
    "for lang in langs:\n",
    "    if lang == 'no_relation': continue\n",
    "    if lang not in lang_dict: lang_dict[lang] = len(lang_dict)\n",
    "\n",
    "print(lang_dict)\n",
    "print(Counter(langs))\n",
    "\n",
    "test_langs                  = []\n",
    "test_lang_dict              = {}\n",
    "for lang in languages:\n",
    "    lang_df \t\t\t\t= pd.read_csv(f'{rel_dir}/{lang}_corpora_test.tsv', sep='\\t')\n",
    "    curr_langs \t\t\t\t= list(lang_df['label'])\n",
    "    test_langs.extend(curr_langs)\n",
    "    \n",
    "for lang in test_langs:\n",
    "    if lang == 'no_relation': continue\n",
    "    if lang not in test_lang_dict: test_lang_dict[lang] = len(test_lang_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>entity_1</th>\n",
       "      <th>entity_2</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3379</td>\n",
       "      <td>손드라 커리</td>\n",
       "      <td>앨런 J. 레비</td>\n",
       "      <td>has-spouse</td>\n",
       "      <td>&lt;e1&gt;손드라 커리&lt;/e1&gt; (, 본명 산드라 마리 커리; 1947년 1월 11일 ...</td>\n",
       "      <td>ko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>583</td>\n",
       "      <td>핌</td>\n",
       "      <td>힐러리</td>\n",
       "      <td>has-sibling</td>\n",
       "      <td>그녀가 은퇴한 후, &lt;e1&gt;핌&lt;/e1&gt;은 그녀의 여동생 &lt;e2&gt;힐러리&lt;/e2&gt;와 함...</td>\n",
       "      <td>ko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22155</td>\n",
       "      <td>미키타니 히로시</td>\n",
       "      <td>일본어</td>\n",
       "      <td>from-country</td>\n",
       "      <td>&lt;e1&gt;미키타니 히로시&lt;/e1&gt;(&lt;e2&gt;일본어&lt;/e2&gt;: 三木谷浩史, 1965년 3...</td>\n",
       "      <td>ko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13410</td>\n",
       "      <td>중앙마케도니아</td>\n",
       "      <td>그리스의 행정 구역</td>\n",
       "      <td>has-type</td>\n",
       "      <td>&lt;e1&gt;중앙마케도니아&lt;/e1&gt;주(그리스어: Περιφέρεια Κεντρικής Μ...</td>\n",
       "      <td>ko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21978</td>\n",
       "      <td>신동엽</td>\n",
       "      <td>대한민국</td>\n",
       "      <td>from-country</td>\n",
       "      <td>&lt;e1&gt;신동엽&lt;/e1&gt;(申東曄, 1930년 8월 18일 ~ 1969년 4월 7일)은...</td>\n",
       "      <td>ko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18706</th>\n",
       "      <td>8320</td>\n",
       "      <td>조지 리처</td>\n",
       "      <td>미국</td>\n",
       "      <td>birth-place</td>\n",
       "      <td>&lt;e1&gt;조지 리처&lt;/e1&gt;(영어: George Ritzer, 1940년 ~ )는 &lt;...</td>\n",
       "      <td>ko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18707</th>\n",
       "      <td>후타바 아키코가수has-occupationko</td>\n",
       "      <td>후타바 아키코</td>\n",
       "      <td>가수</td>\n",
       "      <td>has-occupation</td>\n",
       "      <td>&lt;e1&gt;후타바 아키코&lt;/e1&gt;(일본어: 二葉あき子, 1915년 2월 2일 ~ 201...</td>\n",
       "      <td>ko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18708</th>\n",
       "      <td>류수영배우has-occupationko</td>\n",
       "      <td>류수영</td>\n",
       "      <td>배우</td>\n",
       "      <td>has-occupation</td>\n",
       "      <td>&lt;e1&gt;류수영&lt;/e1&gt;(, 1979년 9월 5일 ~ )은 대한민국의 &lt;e2&gt;배우&lt;/...</td>\n",
       "      <td>ko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18709</th>\n",
       "      <td>3726</td>\n",
       "      <td>니콘</td>\n",
       "      <td>코닥</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>디지털 이미지 센서를 장착한 맞춤 카메라가 &lt;e1&gt;니콘&lt;/e1&gt; F3 차체에 장착되...</td>\n",
       "      <td>ko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18710</th>\n",
       "      <td>297</td>\n",
       "      <td>아드리안 오테로</td>\n",
       "      <td>멤피스 라 블루세라</td>\n",
       "      <td>is-member-of</td>\n",
       "      <td>&lt;e1&gt;아드리안 오테로&lt;/e1&gt; (Adrián Otero, 아르헨티나 부에노스 아이...</td>\n",
       "      <td>ko</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18711 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id  entity_1    entity_2           label  \\\n",
       "0                           3379    손드라 커리    앨런 J. 레비      has-spouse   \n",
       "1                            583         핌         힐러리     has-sibling   \n",
       "2                          22155  미키타니 히로시         일본어    from-country   \n",
       "3                          13410   중앙마케도니아  그리스의 행정 구역        has-type   \n",
       "4                          21978       신동엽        대한민국    from-country   \n",
       "...                          ...       ...         ...             ...   \n",
       "18706                       8320     조지 리처          미국     birth-place   \n",
       "18707  후타바 아키코가수has-occupationko   후타바 아키코          가수  has-occupation   \n",
       "18708      류수영배우has-occupationko       류수영          배우  has-occupation   \n",
       "18709                       3726        니콘          코닥     no_relation   \n",
       "18710                        297  아드리안 오테로  멤피스 라 블루세라    is-member-of   \n",
       "\n",
       "                                                    text lang  \n",
       "0      <e1>손드라 커리</e1> (, 본명 산드라 마리 커리; 1947년 1월 11일 ...   ko  \n",
       "1      그녀가 은퇴한 후, <e1>핌</e1>은 그녀의 여동생 <e2>힐러리</e2>와 함...   ko  \n",
       "2      <e1>미키타니 히로시</e1>(<e2>일본어</e2>: 三木谷浩史, 1965년 3...   ko  \n",
       "3      <e1>중앙마케도니아</e1>주(그리스어: Περιφέρεια Κεντρικής Μ...   ko  \n",
       "4      <e1>신동엽</e1>(申東曄, 1930년 8월 18일 ~ 1969년 4월 7일)은...   ko  \n",
       "...                                                  ...  ...  \n",
       "18706  <e1>조지 리처</e1>(영어: George Ritzer, 1940년 ~ )는 <...   ko  \n",
       "18707  <e1>후타바 아키코</e1>(일본어: 二葉あき子, 1915년 2월 2일 ~ 201...   ko  \n",
       "18708  <e1>류수영</e1>(, 1979년 9월 5일 ~ )은 대한민국의 <e2>배우</...   ko  \n",
       "18709  디지털 이미지 센서를 장착한 맞춤 카메라가 <e1>니콘</e1> F3 차체에 장착되...   ko  \n",
       "18710  <e1>아드리안 오테로</e1> (Adrián Otero, 아르헨티나 부에노스 아이...   ko  \n",
       "\n",
       "[18711 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "langs                       = ['ko']\n",
    "\n",
    "for lang in langs:\n",
    "    lang_df \t\t\t\t= pd.read_csv(f'{rel_dir}/{lang}_corpora_train.tsv', sep='\\t')\n",
    "    \n",
    "    # lang_df[lang_df.label!='no_relation']\n",
    "    # L                       = len(lang_df)\n",
    "    # L2                      = len(lang_df.drop_duplicates())\n",
    "    # test_lang_df            = pd.read_csv(f'{rel_dir}/{lang}_corpora_test.tsv', sep='\\t')\n",
    "    # test_lang_df \t\t\t= pd.read_csv(f'{rel_dir}/{lang}_corpora_test.tsv', sep='\\t')\n",
    "    # test_lang_df[test_lang_df.label!='no_relation']\n",
    "    # lang_sents              = lang_df[['text','label']].values.tolist()\n",
    "    # test_lang_sents         = test_lang_df[['text','label']].values.tolist()\n",
    "    # cnt                     = 0\n",
    "    # for sent in test_lang_sents:\n",
    "    #     if sent in lang_sents:\n",
    "    #         cnt +=1\n",
    "\n",
    "display(lang_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../IndoRE_checkpoints/en_en-model_mbert-dep_0-el_0-mtl_1.0-seed_4-lr_5e-06',\n",
      " '../IndoRE_checkpoints/te_te-model_mbert-dep_1-el_0-mtl_0.5-seed_0-lr_5e-06',\n",
      " '../IndoRE_checkpoints/en_en-model_mbert-dep_1-el_0-mtl_1.0-seed_3-lr_5e-06',\n",
      " '../IndoRE_checkpoints/te_te-model_mbert-dep_0-el_0-mtl_1.0-seed_3-lr_5e-06',\n",
      " '../IndoRE_checkpoints/en_en-model_mbert-dep_0-el_0-mtl_0.5-seed_0-lr_5e-06',\n",
      " '../IndoRE_checkpoints/te_te-model_mbert-dep_1-el_0-mtl_1.0-seed_4-lr_5e-06',\n",
      " '../IndoRE_checkpoints/hi_hi-model_mbert-dep_0-el_0-mtl_0.5-seed_1-lr_5e-06',\n",
      " '../IndoRE_checkpoints/hi_hi-model_mbert-dep_1-el_0-mtl_1.0-seed_2-lr_5e-06',\n",
      " '../IndoRE_checkpoints/te_te-model_mbert-dep_0-el_0-mtl_1.0-seed_4-lr_5e-06',\n",
      " '../IndoRE_checkpoints/en_en-model_mbert-dep_1-el_0-mtl_0.5-seed_0-lr_5e-06',\n",
      " '../IndoRE_checkpoints/te_te-model_mbert-dep_1-el_0-mtl_1.0-seed_3-lr_5e-06',\n",
      " '../IndoRE_checkpoints/en_en-model_mbert-dep_0-el_0-mtl_1.0-seed_3-lr_5e-06',\n",
      " '../IndoRE_checkpoints/te_te-model_mbert-dep_0-el_0-mtl_0.5-seed_0-lr_5e-06',\n",
      " '../IndoRE_checkpoints/en_en-model_mbert-dep_1-el_0-mtl_1.0-seed_4-lr_5e-06',\n",
      " '../IndoRE_checkpoints/hi_hi-model_mbert-dep_0-el_0-mtl_1.0-seed_2-lr_5e-06',\n",
      " '../IndoRE_checkpoints/hi_hi-model_mbert-dep_1-el_0-mtl_0.5-seed_1-lr_5e-06',\n",
      " '../IndoRE_checkpoints/hi_hi-model_mbert-dep_0-el_0-mtl_0.5-seed_0-lr_5e-06',\n",
      " '../IndoRE_checkpoints/hi_hi-model_mbert-dep_0-el_0-mtl_1.0-seed_4-lr_5e-06',\n",
      " '../IndoRE_checkpoints/hi_hi-model_mbert-dep_1-el_0-mtl_1.0-seed_3-lr_5e-06',\n",
      " '../IndoRE_checkpoints/en_en-model_mbert-dep_1-el_0-mtl_1.0-seed_2-lr_5e-06',\n",
      " '../IndoRE_checkpoints/te_te-model_mbert-dep_1-el_0-mtl_0.5-seed_1-lr_5e-06',\n",
      " '../IndoRE_checkpoints/en_en-model_mbert-dep_0-el_0-mtl_0.5-seed_1-lr_5e-06',\n",
      " '../IndoRE_checkpoints/te_te-model_mbert-dep_0-el_0-mtl_1.0-seed_2-lr_5e-06',\n",
      " '../IndoRE_checkpoints/hi_hi-model_mbert-dep_0-el_0-mtl_1.0-seed_3-lr_5e-06',\n",
      " '../IndoRE_checkpoints/hi_hi-model_mbert-dep_1-el_0-mtl_1.0-seed_4-lr_5e-06',\n",
      " '../IndoRE_checkpoints/hi_hi-model_mbert-dep_1-el_0-mtl_0.5-seed_0-lr_5e-06',\n",
      " '../IndoRE_checkpoints/te_te-model_mbert-dep_1-el_0-mtl_1.0-seed_2-lr_5e-06',\n",
      " '../IndoRE_checkpoints/en_en-model_mbert-dep_1-el_0-mtl_0.5-seed_1-lr_5e-06',\n",
      " '../IndoRE_checkpoints/te_te-model_mbert-dep_0-el_0-mtl_0.5-seed_1-lr_5e-06',\n",
      " '../IndoRE_checkpoints/en_en-model_mbert-dep_0-el_0-mtl_1.0-seed_2-lr_5e-06',\n",
      " '../IndoRE_checkpoints/te_te-model_mbert-dep_1-el_0-mtl_1.0-seed_1-lr_5e-06',\n",
      " '../IndoRE_checkpoints/en_en-model_mbert-dep_1-el_0-mtl_0.5-seed_2-lr_5e-06',\n",
      " '../IndoRE_checkpoints/te_te-model_mbert-dep_0-el_0-mtl_0.5-seed_2-lr_5e-06',\n",
      " '../IndoRE_checkpoints/en_en-model_mbert-dep_0-el_0-mtl_1.0-seed_1-lr_5e-06',\n",
      " '../IndoRE_checkpoints/hi_hi-model_mbert-dep_0-el_0-mtl_1.0-seed_0-lr_5e-06',\n",
      " '../IndoRE_checkpoints/hi_hi-model_mbert-dep_1-el_0-mtl_0.5-seed_3-lr_5e-06',\n",
      " '../IndoRE_checkpoints/hi_hi-model_mbert-dep_0-el_0-mtl_0.5-seed_4-lr_5e-06',\n",
      " '../IndoRE_checkpoints/en_en-model_mbert-dep_1-el_0-mtl_1.0-seed_1-lr_5e-06',\n",
      " '../IndoRE_checkpoints/te_te-model_mbert-dep_1-el_0-mtl_0.5-seed_2-lr_5e-06',\n",
      " '../IndoRE_checkpoints/en_en-model_mbert-dep_0-el_0-mtl_0.5-seed_2-lr_5e-06',\n",
      " '../IndoRE_checkpoints/te_te-model_mbert-dep_0-el_0-mtl_1.0-seed_1-lr_5e-06',\n",
      " '../IndoRE_checkpoints/hi_hi-model_mbert-dep_1-el_0-mtl_0.5-seed_4-lr_5e-06',\n",
      " '../IndoRE_checkpoints/hi_hi-model_mbert-dep_0-el_0-mtl_0.5-seed_3-lr_5e-06',\n",
      " '../IndoRE_checkpoints/hi_hi-model_mbert-dep_1-el_0-mtl_1.0-seed_0-lr_5e-06',\n",
      " '../IndoRE_checkpoints/hi_hi-model_mbert-dep_0-el_0-mtl_1.0-seed_1-lr_5e-06',\n",
      " '../IndoRE_checkpoints/hi_hi-model_mbert-dep_1-el_0-mtl_0.5-seed_2-lr_5e-06',\n",
      " '../IndoRE_checkpoints/en_en-model_mbert-dep_1-el_0-mtl_0.5-seed_3-lr_5e-06',\n",
      " '../IndoRE_checkpoints/te_te-model_mbert-dep_1-el_0-mtl_1.0-seed_0-lr_5e-06',\n",
      " '../IndoRE_checkpoints/en_en-model_mbert-dep_0-el_0-mtl_0.5-seed_4-lr_5e-06',\n",
      " '../IndoRE_checkpoints/te_te-model_mbert-dep_1-el_0-mtl_0.5-seed_4-lr_5e-06',\n",
      " '../IndoRE_checkpoints/en_en-model_mbert-dep_0-el_0-mtl_1.0-seed_0-lr_5e-06',\n",
      " '../IndoRE_checkpoints/te_te-model_mbert-dep_0-el_0-mtl_0.5-seed_3-lr_5e-06',\n",
      " '../IndoRE_checkpoints/hi_hi-model_mbert-dep_0-el_0-mtl_0.5-seed_2-lr_5e-06',\n",
      " '../IndoRE_checkpoints/hi_hi-model_mbert-dep_1-el_0-mtl_1.0-seed_1-lr_5e-06',\n",
      " '../IndoRE_checkpoints/te_te-model_mbert-dep_1-el_0-mtl_0.5-seed_3-lr_5e-06',\n",
      " '../IndoRE_checkpoints/en_en-model_mbert-dep_1-el_0-mtl_1.0-seed_0-lr_5e-06',\n",
      " '../IndoRE_checkpoints/te_te-model_mbert-dep_0-el_0-mtl_0.5-seed_4-lr_5e-06',\n",
      " '../IndoRE_checkpoints/en_en-model_mbert-dep_1-el_0-mtl_0.5-seed_4-lr_5e-06',\n",
      " '../IndoRE_checkpoints/te_te-model_mbert-dep_0-el_0-mtl_1.0-seed_0-lr_5e-06',\n",
      " '../IndoRE_checkpoints/en_en-model_mbert-dep_0-el_0-mtl_0.5-seed_3-lr_5e-06']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from pprint import pprint\n",
    "\n",
    "ckpts = glob(f'../IndoRE_checkpoints/*')\n",
    "for ckpt in ckpts:\n",
    "    if 'dep_0' in ckpt:\n",
    "        os.rename(ckpt, ckpt.replace('dep_0-el','dep_0-gnn-depth_5-el'))\n",
    "# for ckpt in ckpts:\n",
    "#     if 'gnn_depth'\n",
    "\n",
    "pprint(ckpts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6963it [00:00, 31916.95it/s]?, ?it/s]\n",
      "8486it [00:00, 41343.16it/s]00:00,  3.99it/s]\n",
      "8154it [00:00, 34012.74it/s]00:00,  4.27it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00,  3.95it/s]\n"
     ]
    }
   ],
   "source": [
    "from helper import *\n",
    "\n",
    "data_dir    = '/data/multilingual_KGQA/IndoRE/data/'\n",
    "langs       = ['hindi','english','bengali','telugu']\n",
    "lang_codes = {'hi':'hindi', 'en':'english','te': 'telugu'}\n",
    "\n",
    "split_langs \t=    f'{data_dir}/lang_sents.dill'\n",
    "uniq_dict       =   ddict(dict)\n",
    "rel_dict        =   {}\n",
    "\n",
    "for line in open(f'{data_dir}/kge_rels.del').readlines():\n",
    "    id, rel_name =  line.strip().split('\\t')   \n",
    "    rel_dict[rel_name] = int(id)\n",
    "\n",
    "ent_embs        =   load_pickle(f'{data_dir}/entity_embeddings.pkl')\n",
    "\n",
    "for lang in tqdm(lang_codes):\n",
    "    cnt                                     =    0\n",
    "    lines \t\t                            =    open(f'{data_dir}/{lang_codes[lang]}_indore.tsv').readlines()\n",
    "    graph_data                              =    open(f'{data_dir}/{lang_codes[lang]}_indore_el_graph.json').readlines()\n",
    "    \n",
    "    for line, elem in tqdm(zip(lines, graph_data)):\n",
    "        elem                                =    json.loads(elem)\n",
    "        rel, sent, ent_1, ent_2             =    line.strip().split('\\t')\n",
    "        graph_data                          =    elem['graph']\n",
    "        graph_ent1, graph_ent2              =    elem['entity1'], elem['entity2']\n",
    "        uniq_dict[lang][sent]               =    (elem['graph'], elem['entity1'],elem['entity2'], elem['nodes'])\n",
    "    \n",
    "    # print(lang, cnt)\n",
    "\n",
    "# dump_pickle(uniq_dict, f'{data_dir}/ent2_embs.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5570/5570 [00:01<00:00, 4380.68it/s]\n",
      "100%|██████████| 697/697 [00:00<00:00, 9539.04it/s]\n",
      "100%|██████████| 696/696 [00:00<00:00, 6651.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dill dumped successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6523/6523 [00:01<00:00, 5235.31it/s]\n",
      "100%|██████████| 816/816 [00:00<00:00, 9429.56it/s]\n",
      "100%|██████████| 815/815 [00:00<00:00, 9409.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dill dumped successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6788/6788 [00:01<00:00, 6086.04it/s]\n",
      "100%|██████████| 849/849 [00:00<00:00, 9969.05it/s]\n",
      "100%|██████████| 849/849 [00:00<00:00, 8930.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dill dumped successfully\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "for lang in ['hi','te','en']:\n",
    "    rels_data = load_dill(f'{data_dir}/{lang}_ent_rels.dill')\n",
    "    lang_cnts = 0\n",
    "    data   \t\t\t        \t\t\t= ddict(lambda: ddict(list))\n",
    "    for split in ['train','test','dev']:\n",
    "        for elem in tqdm(rels_data[split]['rels']):\n",
    "            curr_data                = elem\n",
    "            sent, arg1_emb, arg2_emb = elem['orig_sent'], elem['arg1_emb'], elem['arg2_emb']\n",
    "            if sent not in uniq_dict[lang]: \n",
    "                lang_cnts+=1\n",
    "            \n",
    "            x, edge_index, edge_type, n1_mask, n2_mask          = [], [[],[]], [], [],[]\n",
    "            graph_data, e1, e2, nodes = uniq_dict[lang][sent]\n",
    "            \n",
    "            node_dict     = {}\n",
    "            for i, node in enumerate(nodes): node_dict[node]    = i\n",
    "            \n",
    "            x             = [torch.FloatTensor(ent_embs[n]) for n in nodes]\n",
    "            for item in graph_data:\n",
    "                n1, rel, n2 = item\n",
    "                edge_index[0].append(node_dict[n1])\n",
    "                edge_index[1].append(node_dict[n2])\n",
    "                edge_type.append(rel_dict[rel])\n",
    "                \n",
    "            n1_mask       = [1 if n==e1 else 0 for n in nodes ]\n",
    "            n2_mask       = [1 if n==e2 else 0 for n in nodes ]\n",
    "            \n",
    "            if len(x) == 0:\n",
    "                x       = [torch.FloatTensor(elem['arg1_emb']),torch.FloatTensor(elem['arg2_emb'])]\n",
    "                n1_mask = [1,0]\n",
    "                n2_mask = [0,1]\n",
    "            \n",
    "            x, edge_index, edge_type, n1_mask, n2_mask\t= torch.stack(x, dim=0), torch.LongTensor(edge_index), torch.LongTensor(edge_type), torch.LongTensor(n1_mask),torch.LongTensor(n2_mask)\n",
    "            ent_data \t\t\t\t\t\t\t\t\t= Data(x=x, edge_index= edge_index, edge_type=edge_type, n1_mask=n1_mask, n2_mask=n2_mask)\n",
    "            \n",
    "                \n",
    "            curr_data['ent_data']                           = ent_data\n",
    "            data[split]['rels'].append(curr_data)\n",
    "            \n",
    "    dump_dill(data, f'{data_dir}/{lang}_ent2_rels.dill')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Q2603334', 'P155', 'Q2530501'], ['Q1033304', 'P31', 'Q2603334'], ['Q2530501', 'P530', 'Q2603334']]\n"
     ]
    }
   ],
   "source": [
    "graph_data, e1, e2, nodes = uniq_dict[lang][sent]\n",
    "print(graph_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Q2530501', 'Q1033304', 'Q2603334']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([282, 433, 564])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n2_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 365/365 [00:00<00:00, 620057.09it/s]\n"
     ]
    }
   ],
   "source": [
    "deprel_dict \t\t\t\t\t\t\t\t\t\t= \tload_deprels(enhanced=False)\n",
    "entity_dict             \t\t\t\t\t\t\t= \tload_pickle(f'{data_dir}/ents.pkl')\n",
    "relation_dict          \t\t\t\t\t\t\t\t= \tload_pickle(f'{data_dir}/rels.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relation_dict          \t\t\t\t\t\t\t\t= \tload_pickle(f'{data_dir}/rels.pkl'.replace('IndoRE','SMILER'))\n",
    "len(relation_dict)\n",
    "len(deprel_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3585/3585 [00:00<00:00, 43091.18it/s]\n",
      "100%|██████████| 448/448 [00:00<00:00, 80670.08it/s]\n",
      "100%|██████████| 540/540 [00:00<00:00, 67881.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sv\n",
      "Counter({34: 14889, 21: 13744, 6: 11004, 36: 6587, 37: 6587, 22: 5704, 3: 5683, 23: 5448, 25: 4805, 10: 4553, 14: 4462, 11: 3653, 18: 2975, 4: 2413, 7: 1651, 24: 1504, 0: 1341, 2: 781, 9: 490, 27: 338, 20: 297, 29: 128, 5: 119, 31: 115, 1: 103, 26: 48, 8: 27, 17: 15, 19: 14, 12: 11, 33: 6, 15: 2})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7967/7967 [00:00<00:00, 51661.13it/s]\n",
      "100%|██████████| 994/994 [00:00<00:00, 79824.20it/s]\n",
      "100%|██████████| 2496/2496 [00:00<00:00, 77455.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it\n",
      "Counter({34: 52812, 18: 39926, 6: 35859, 14: 35289, 21: 34235, 3: 17143, 36: 12887, 37: 12887, 25: 12870, 22: 12455, 11: 11719, 4: 9301, 10: 8503, 0: 7665, 7: 5778, 23: 3614, 5: 2907, 2: 1659, 9: 1505, 24: 1158, 27: 657, 1: 463, 20: 440, 29: 350, 17: 319, 31: 207, 8: 76, 13: 56, 19: 40, 15: 28, 12: 11, 28: 7, 33: 6})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7982/7982 [00:00<00:00, 35181.77it/s]\n",
      "100%|██████████| 999/999 [00:00<00:00, 65475.58it/s]\n",
      "100%|██████████| 1222/1222 [00:00<00:00, 69250.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "es\n",
      "Counter({34: 60364, 6: 48887, 18: 40436, 14: 27469, 3: 21436, 21: 20023, 10: 16113, 4: 15104, 9: 14744, 22: 11856, 36: 11691, 37: 11691, 11: 9906, 25: 8889, 7: 8672, 24: 6472, 2: 3144, 20: 2758, 23: 2288, 0: 2205, 1: 874, 31: 866, 5: 736, 17: 621, 29: 325, 8: 115, 13: 49, 19: 47, 12: 41, 27: 32, 26: 1})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 773/773 [00:00<00:00, 4980.11it/s]\n",
      "100%|██████████| 97/97 [00:00<00:00, 54875.57it/s]\n",
      "100%|██████████| 116/116 [00:00<00:00, 58931.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uk\n",
      "Counter({34: 7683, 21: 3467, 3: 2645, 18: 2334, 10: 2094, 6: 1581, 36: 1197, 37: 1197, 7: 1184, 22: 1090, 27: 1035, 25: 925, 4: 640, 24: 335, 0: 227, 23: 218, 2: 204, 20: 189, 14: 187, 9: 179, 26: 126, 11: 64, 1: 46, 29: 46, 15: 21, 8: 19, 33: 15, 31: 13, 19: 8, 28: 3, 17: 3, 12: 1, 5: 1})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2099/2099 [00:00<00:00, 18439.92it/s]\n",
      "100%|██████████| 262/262 [00:00<00:00, 65085.74it/s]\n",
      "100%|██████████| 317/317 [00:00<00:00, 63531.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fa\n",
      "Counter({34: 10228, 21: 10111, 18: 5933, 6: 4910, 22: 3683, 25: 3549, 3: 3514, 36: 3012, 37: 3012, 10: 2868, 11: 2569, 7: 1699, 23: 1605, 4: 1459, 0: 1325, 9: 1266, 20: 873, 24: 585, 5: 482, 2: 221, 8: 195, 29: 191, 14: 178, 1: 74, 13: 32, 31: 8, 12: 6, 28: 6})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7984/7984 [00:00<00:00, 54290.71it/s]\n",
      "100%|██████████| 998/998 [00:00<00:00, 79480.41it/s]\n",
      "100%|██████████| 2046/2046 [00:00<00:00, 73701.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de\n",
      "Counter({34: 56302, 21: 29173, 18: 27984, 4: 26887, 6: 21892, 14: 21798, 36: 17695, 37: 17695, 3: 14322, 10: 9882, 22: 8999, 11: 8654, 25: 5792, 7: 5659, 13: 5487, 23: 4790, 2: 3708, 0: 1907, 9: 1539, 24: 1443, 5: 921, 29: 303, 20: 174, 1: 104, 19: 81, 27: 71, 17: 23, 31: 21, 8: 15, 12: 9})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7987/7987 [00:00<00:00, 42971.57it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 70613.56it/s]\n",
      "100%|██████████| 1343/1343 [00:00<00:00, 76429.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pl\n",
      "Counter({34: 57712, 21: 37608, 3: 28086, 6: 22936, 18: 20845, 10: 17337, 36: 12349, 37: 12349, 4: 9585, 25: 6467, 7: 3620, 22: 3288, 0: 2963, 23: 1966, 27: 1793, 24: 1565, 2: 1190, 19: 925, 31: 723, 11: 488, 17: 409, 20: 398, 14: 273, 33: 229, 5: 157, 26: 74, 29: 73, 1: 46, 28: 27, 8: 14, 12: 1, 15: 1})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7439/7439 [00:00<00:00, 41293.52it/s]\n",
      "100%|██████████| 930/930 [00:00<00:00, 66308.03it/s]\n",
      "100%|██████████| 1121/1121 [00:00<00:00, 74441.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ar\n",
      "Counter({21: 54930, 34: 45585, 6: 30369, 25: 23454, 3: 19747, 7: 19181, 10: 18857, 22: 12065, 23: 11267, 36: 9546, 37: 9546, 13: 8227, 27: 4952, 18: 3351, 0: 2256, 24: 2126, 31: 857, 4: 655, 20: 443, 29: 384, 14: 347, 5: 256, 1: 173, 26: 164, 2: 160, 11: 160, 8: 74, 12: 11, 16: 8, 19: 2})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7973/7973 [00:00<00:00, 36697.14it/s]\n",
      "100%|██████████| 993/993 [00:00<00:00, 79623.46it/s]\n",
      "100%|██████████| 1878/1878 [00:00<00:00, 85605.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pt\n",
      "Counter({34: 55768, 6: 51146, 21: 39630, 14: 30614, 18: 27343, 10: 15201, 3: 14914, 36: 13229, 37: 13229, 22: 12902, 4: 11932, 11: 10889, 25: 10552, 0: 7462, 7: 6583, 27: 4270, 23: 2688, 2: 2105, 24: 1890, 29: 1260, 5: 553, 1: 474, 20: 352, 31: 335, 17: 190, 8: 52, 9: 34, 19: 24, 33: 21, 12: 16, 15: 2, 28: 1})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7981/7981 [00:00<00:00, 35812.66it/s]\n",
      "100%|██████████| 999/999 [00:00<00:00, 72499.52it/s]\n",
      "100%|██████████| 2241/2241 [00:00<00:00, 63999.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fr\n",
      "Counter({34: 49828, 6: 43587, 21: 42035, 14: 34833, 18: 31540, 25: 27239, 3: 17130, 36: 12866, 37: 12866, 22: 12553, 10: 12314, 0: 12038, 11: 11462, 4: 8678, 7: 8240, 24: 5586, 2: 1634, 1: 1197, 13: 1174, 29: 1064, 5: 1057, 12: 1041, 20: 808, 23: 771, 31: 326, 17: 88, 27: 79, 8: 72, 26: 5})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7992/7992 [00:00<00:00, 33139.84it/s]\n",
      "100%|██████████| 998/998 [00:00<00:00, 73676.24it/s]\n",
      "100%|██████████| 1791/1791 [00:00<00:00, 75701.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nl\n",
      "Counter({34: 46919, 18: 31799, 14: 24936, 21: 22306, 6: 20690, 36: 15366, 37: 15366, 22: 15253, 3: 14863, 10: 12112, 4: 11170, 11: 10353, 25: 7533, 7: 6180, 23: 5571, 24: 5323, 27: 3179, 0: 2453, 31: 1784, 9: 1541, 2: 1265, 20: 1082, 5: 904, 1: 486, 29: 464, 17: 43, 26: 25, 8: 17, 19: 12, 12: 3})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7916/7916 [00:00<00:00, 42668.45it/s]\n",
      "100%|██████████| 995/995 [00:00<00:00, 73439.25it/s]\n",
      "100%|██████████| 6402/6402 [00:00<00:00, 84397.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en\n",
      "Counter({34: 70528, 6: 40322, 9: 34922, 21: 30683, 14: 30585, 3: 28618, 18: 28447, 25: 21481, 22: 21208, 36: 18959, 37: 18959, 10: 17717, 11: 15498, 4: 14722, 0: 13014, 7: 11807, 23: 6533, 24: 4501, 2: 4001, 5: 2318, 27: 1162, 1: 1130, 20: 1022, 13: 974, 29: 905, 31: 544, 8: 159, 12: 158, 33: 152, 15: 49, 17: 30, 26: 18, 19: 13, 28: 10, 16: 3, 32: 3})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5114/5114 [00:00<00:00, 18669.14it/s]\n",
      "100%|██████████| 639/639 [00:00<00:00, 75209.35it/s]\n",
      "100%|██████████| 771/771 [00:00<00:00, 83842.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ru\n",
      "Counter({34: 54366, 10: 18235, 23: 13675, 18: 12682, 21: 12386, 3: 12272, 4: 9702, 6: 9633, 36: 8874, 37: 8874, 25: 6028, 22: 5794, 7: 3885, 27: 2651, 0: 1135, 24: 673, 2: 642, 14: 449, 20: 382, 31: 268, 9: 190, 19: 168, 1: 167, 11: 103, 5: 101, 29: 56, 26: 47, 8: 22, 15: 9, 12: 7})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from helper import *\n",
    "data_dir  = '/data/multilingual_KGQA/SMILER/data/'\n",
    "\n",
    "dep_nums = ddict(list)\n",
    "\n",
    "files = os.listdir(data_dir)\n",
    "for file in files:\n",
    "    if file.endswith('rels.dill') is False: continue\n",
    "    lang = file.split('_')[0]\n",
    "    data = load_dill(f'{data_dir}/{file}')\n",
    "    for split in ['train','dev','test']:\n",
    "        split_rels = data[split]['rels']\n",
    "        for elem in tqdm(split_rels):\n",
    "            dep_data   = elem['dep_data']\n",
    "            edge_types = list(dep_data.edge_type.cpu().detach().numpy())\n",
    "            dep_nums[lang].extend(edge_types)\n",
    "\n",
    "    print(lang)    \n",
    "    print(Counter(dep_nums[lang]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>Baseline</th>\n",
       "      <th>Dependency</th>\n",
       "      <th>Num_rels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ru</td>\n",
       "      <td>0.954926</td>\n",
       "      <td>0.971384</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ar</td>\n",
       "      <td>0.965955</td>\n",
       "      <td>0.962367</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>uk</td>\n",
       "      <td>0.933560</td>\n",
       "      <td>0.914561</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fa</td>\n",
       "      <td>0.838302</td>\n",
       "      <td>0.801976</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nl</td>\n",
       "      <td>0.785570</td>\n",
       "      <td>0.793116</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sv</td>\n",
       "      <td>0.749038</td>\n",
       "      <td>0.726521</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>en</td>\n",
       "      <td>0.842278</td>\n",
       "      <td>0.836242</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>pt</td>\n",
       "      <td>0.810024</td>\n",
       "      <td>0.803250</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>de</td>\n",
       "      <td>0.824906</td>\n",
       "      <td>0.833759</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Language  Baseline  Dependency  Num_rels\n",
       "0       ru  0.954926    0.971384         8\n",
       "1       ar  0.965955    0.962367         8\n",
       "2       uk  0.933560    0.914561         6\n",
       "3       fa  0.838302    0.801976         5\n",
       "4       nl  0.785570    0.793116        21\n",
       "5       sv  0.749038    0.726521        22\n",
       "6       en  0.842278    0.836242        36\n",
       "7       pt  0.810024    0.803250        22\n",
       "8       de  0.824906    0.833759        22"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Draw code for SMILER data\n",
    "from helper import *\n",
    "from sklearn.metrics import f1_score\n",
    "data_dir ='../predictions_SMILER/'\n",
    "langs    = list(set([file.split('_')[0] for file in os.listdir(data_dir)]))\n",
    "\n",
    "lang_dict = ddict(list)\n",
    "for lang in langs:\n",
    "    bl   = load_pickle(f'{data_dir}/{lang}_{lang}-dep_0-el_0-gnn-depth_5.pkl')\n",
    "    dep  = load_pickle(f'{data_dir}/{lang}_{lang}-dep_1-el_0-gnn-depth_2.pkl')\n",
    "    \n",
    "    bl_true = [bl[k]['true_rel'] for k in bl.keys()]    \n",
    "    bl_pred = [bl[k]['pred_rel'] for k in bl.keys()]    \n",
    "    \n",
    "    dep_true = [dep[k]['true_rel'] for k in dep.keys()]    \n",
    "    dep_pred = [dep[k]['pred_rel'] for k in dep.keys()]    \n",
    "    \n",
    "    bl_f1    = \tf1_score(bl_true,bl_pred, average=\"macro\")\n",
    "    dep_f1    = f1_score(dep_true,dep_pred, average=\"macro\")\n",
    "    \n",
    "    lang_dict[\"Language\"].append(lang)\n",
    "    lang_dict[\"Baseline\"].append(bl_f1)\n",
    "    lang_dict[\"Dependency\"].append(dep_f1)\n",
    "    lang_dict['Num_rels'].append(len(set(bl_true)))\n",
    "    # nd = [(nd_data[k][\"sent_len\"], int(nd_data[k][\"pred_rel\"] == nd_data[k][\"true_rel\"])) for k in nd_data.keys() if \"pred_rel\" in nd_data[k]]\n",
    "    # d2 = [(d2_data[k][\"sent_len\"], int(d2_data[k][\"pred_rel\"] == d2_data[k][\"true_rel\"])) for k in d2_data.keys() if \"pred_rel\" in nd_data[k]]\n",
    "    \n",
    "lang_df = pd.DataFrame(lang_dict)\n",
    "display(lang_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Src_lang</th>\n",
       "      <th>ar</th>\n",
       "      <th>de</th>\n",
       "      <th>en</th>\n",
       "      <th>fa</th>\n",
       "      <th>nl</th>\n",
       "      <th>pt</th>\n",
       "      <th>ru</th>\n",
       "      <th>sv</th>\n",
       "      <th>uk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ar</td>\n",
       "      <td>-</td>\n",
       "      <td>-1.56</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>-5.01</td>\n",
       "      <td>-0.63</td>\n",
       "      <td>-2.72</td>\n",
       "      <td>-2.64</td>\n",
       "      <td>-8.5</td>\n",
       "      <td>1.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>de</td>\n",
       "      <td>-10.74</td>\n",
       "      <td>-</td>\n",
       "      <td>-1.26</td>\n",
       "      <td>-12.03</td>\n",
       "      <td>-1.73</td>\n",
       "      <td>1.04</td>\n",
       "      <td>-12.45</td>\n",
       "      <td>-7.82</td>\n",
       "      <td>-22.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>en</td>\n",
       "      <td>-5.72</td>\n",
       "      <td>-1.08</td>\n",
       "      <td>-</td>\n",
       "      <td>-9.51</td>\n",
       "      <td>0.61</td>\n",
       "      <td>4.87</td>\n",
       "      <td>1.31</td>\n",
       "      <td>8.11</td>\n",
       "      <td>6.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fa</td>\n",
       "      <td>9.16</td>\n",
       "      <td>13.5</td>\n",
       "      <td>4.47</td>\n",
       "      <td>-</td>\n",
       "      <td>9.44</td>\n",
       "      <td>0.59</td>\n",
       "      <td>-3.75</td>\n",
       "      <td>1.56</td>\n",
       "      <td>8.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nl</td>\n",
       "      <td>-3.1</td>\n",
       "      <td>-1.85</td>\n",
       "      <td>0.25</td>\n",
       "      <td>25.94</td>\n",
       "      <td>-</td>\n",
       "      <td>0.78</td>\n",
       "      <td>-3.77</td>\n",
       "      <td>1.66</td>\n",
       "      <td>-19.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pt</td>\n",
       "      <td>-3.53</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-12.25</td>\n",
       "      <td>-3.49</td>\n",
       "      <td>-</td>\n",
       "      <td>-4.17</td>\n",
       "      <td>-1.01</td>\n",
       "      <td>24.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ru</td>\n",
       "      <td>1.12</td>\n",
       "      <td>2.56</td>\n",
       "      <td>-2.21</td>\n",
       "      <td>-14.53</td>\n",
       "      <td>2.08</td>\n",
       "      <td>-5.27</td>\n",
       "      <td>-</td>\n",
       "      <td>-4.52</td>\n",
       "      <td>-0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sv</td>\n",
       "      <td>5.61</td>\n",
       "      <td>-3.02</td>\n",
       "      <td>-3.19</td>\n",
       "      <td>-25.83</td>\n",
       "      <td>-2.71</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>-6.55</td>\n",
       "      <td>-</td>\n",
       "      <td>-18.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>uk</td>\n",
       "      <td>3.72</td>\n",
       "      <td>0.29</td>\n",
       "      <td>3.05</td>\n",
       "      <td>-5.27</td>\n",
       "      <td>0.64</td>\n",
       "      <td>-6.24</td>\n",
       "      <td>-1.07</td>\n",
       "      <td>-5.64</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Src_lang     ar    de    en     fa    nl    pt     ru    sv     uk\n",
       "0       ar      - -1.56 -0.36  -5.01 -0.63 -2.72  -2.64  -8.5   1.46\n",
       "1       de -10.74     - -1.26 -12.03 -1.73  1.04 -12.45 -7.82 -22.69\n",
       "2       en  -5.72 -1.08     -  -9.51  0.61  4.87   1.31  8.11   6.32\n",
       "3       fa   9.16  13.5  4.47      -  9.44  0.59  -3.75  1.56   8.02\n",
       "4       nl   -3.1 -1.85  0.25  25.94     -  0.78  -3.77  1.66 -19.53\n",
       "5       pt  -3.53 -0.25   0.7 -12.25 -3.49     -  -4.17 -1.01  24.54\n",
       "6       ru   1.12  2.56 -2.21 -14.53  2.08 -5.27      - -4.52  -0.18\n",
       "7       sv   5.61 -3.02 -3.19 -25.83 -2.71  -0.7  -6.55     - -18.94\n",
       "8       uk   3.72  0.29  3.05  -5.27  0.64 -6.24  -1.07 -5.64      -"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "langs        = sorted(langs)\n",
    "zs_lang_dict = ddict(list)\n",
    "\n",
    "more_rels    = ['de','sv','en','nl']\n",
    "# less_rels    = ['uk','ru','ar','fa']\n",
    "\n",
    "for src_lang in langs:\n",
    "    # if src_lang in ['ru','pt','pl']: continue\n",
    "    zs_lang_dict[\"Src_lang\"].append(src_lang)\n",
    "    for tgt_lang in langs:\n",
    "        # if tgt_lang in ['ru','pt','pl']: continue\n",
    "        if src_lang == tgt_lang:\n",
    "            zs_lang_dict[tgt_lang].append('-')\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            bl   = load_pickle(f'{data_dir}/{src_lang}_{tgt_lang}-dep_0-el_0-gnn-depth_5.pkl')\n",
    "            dep  = load_pickle(f'{data_dir}/{src_lang}_{tgt_lang}-dep_1-el_0-gnn-depth_2.pkl')\n",
    "        except Exception as e:\n",
    "            zs_lang_dict[tgt_lang].append('-')\n",
    "            continue\n",
    "            \n",
    "        bl_true = [bl[k]['true_rel'] for k in bl.keys()]    \n",
    "        bl_pred = [bl[k]['pred_rel'] for k in bl.keys()]    \n",
    "        \n",
    "        dep_true = [dep[k]['true_rel'] for k in dep.keys()]    \n",
    "        dep_pred = [dep[k]['pred_rel'] for k in dep.keys()]    \n",
    "        \n",
    "        bl_f1    = \tf1_score(bl_true,bl_pred, average=\"macro\")\n",
    "        dep_f1    = f1_score(dep_true,dep_pred, average=\"macro\")\n",
    "        \n",
    "        diff      = round(100*(dep_f1 - bl_f1)/bl_f1,2)\n",
    "        zs_lang_dict[tgt_lang].append(diff)\n",
    "        # nd = [(nd_data[k][\"sent_len\"], int(nd_data[k][\"pred_rel\"] == nd_data[k][\"true_rel\"])) for k in nd_data.keys() if \"pred_rel\" in nd_data[k]]\n",
    "        # d2 = [(d2_data[k][\"sent_len\"], int(d2_data[k][\"pred_rel\"] == d2_data[k][\"true_rel\"])) for k in d2_data.keys() if \"pred_rel\" in nd_data[k]]\n",
    "        \n",
    "zs_lang_df = pd.DataFrame(zs_lang_dict)\n",
    "display(zs_lang_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import *\n",
    "data_dir  = '/data/multilingual_KGQA/IndoRE/data/'\n",
    "lex_lang                                            = ddict(list)\n",
    "dep_lang                                            = ddict(list)\n",
    "sent_lang                                           = ddict(list)\n",
    "from igraph import *\n",
    "\n",
    "def get_dep(dep_data):\n",
    "    # dep_data \t\t\t\t\t\t\t\t\t\t  = data[idx]['dep_data']\n",
    "    \n",
    "    edges                                             = dep_data.edge_index.cpu().detach().numpy()\n",
    "    n1_mask, n2_mask                                  = dep_data.n1_mask.cpu().detach().numpy(), dep_data.n2_mask.cpu().detach().numpy() \n",
    "    src_nodes                                         = np.where(n1_mask==1)[0]\n",
    "    tgt_nodes                                         = np.where(n2_mask==1)[0]\n",
    "    graph                                             = Graph(directed=False)\n",
    "    graph.add_vertices(list(range(len(n1_mask))))\n",
    "    graph.add_edges([(n1,n2) for n1,n2 in zip(edges[0],edges[1])])\n",
    "    try:\n",
    "        sp = min([min(path_len) for path_len in graph.shortest_paths(src_nodes, tgt_nodes)])\n",
    "    except Exception as e:\n",
    "        if len(src_nodes)>0 and len(tgt_nodes) >0 : sp =0\n",
    "        else: raise AssertionError\n",
    "        \n",
    "    return sp\n",
    "\n",
    "\n",
    "\n",
    "for lang in ['en','hi','te']:\n",
    "    src_file \t\t\t\t\t\t\t\t\t    = \tf'{data_dir}/{lang}_ent_rels.dill'\n",
    "    src_dataset\t\t\t\t\t\t\t\t\t\t=   load_dill(src_file)\n",
    "    \n",
    "    for split in ['train','test','dev']:\n",
    "        data                                        =   src_dataset[split]['rels']\n",
    "        for idx in range(0, len(data)):\n",
    "            arg1_start, arg2_start \t\t            = data[idx]['arg1_ids'].index(1), data[idx]['arg2_ids'].index(1) \n",
    "            arg1_end, arg2_end \t\t\t            = arg1_start + np.sum(data[idx]['arg1_ids'])-1, arg2_start + np.sum(data[idx]['arg2_ids'])-1\n",
    "            lex_dist                                = max(arg1_start, arg2_start) - min(arg1_end, arg2_end)\n",
    "            lex_lang[lang].append(lex_dist)\n",
    "            sp                                      = get_dep(data[idx]['dep_data'])\n",
    "            dep_lang[lang].append(sp)\n",
    "            sent_lang[lang].append(len(data[idx]['bert_toks']))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en 10.546547254301203 8.0 8.702598712456707\n",
      "hi 19.830532816314808 13.0 21.592042288970966\n",
      "te 37.35271032622026 24.0 38.42987747310311\n",
      "en 3.02828187603111 3.0 1.443785693301471\n",
      "hi 3.686772942697113 3.0 2.169860386211898\n",
      "te 6.077998528329654 6.0 3.6234051143923924\n"
     ]
    }
   ],
   "source": [
    "for lang in lex_lang:\n",
    "    print(lang, np.mean(lex_lang[lang]), np.median(lex_lang[lang]), np.std(lex_lang[lang]))\n",
    "\n",
    "for lang in dep_lang:\n",
    "    print(lang, np.mean(dep_lang[lang]), np.median(dep_lang[lang]), np.std(dep_lang[lang]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f6efef6f8c8d14278940a9f07664ad57ed2cfa7f3294b0a801139dfbd8e181b6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('11737hw')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
